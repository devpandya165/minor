{
  "metadata": {
    "name": "abc",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.DataFrame\r\n\r\n// ---------- VERSIONING CONFIG ----------\r\n\r\n// Change this string when you want a \"fresh\" run with new folders\r\n// e.g. \"v1\", \"v2\", \"exp1\", \"final\", etc.\r\nval runId \u003d \"v2\"\r\n\r\n// Build versioned paths\r\nval outputPath \u003d s\"/home/devpandya/parquet_clean_$runId\"\r\nval ckPath     \u003d s\"/home/devpandya/parquet_clean_ck_$runId\"\r\n\r\nprintln(s\"Using output path: $outputPath\")\r\nprintln(s\"Using checkpoint : $ckPath\")\r\n\r\n// ---------- 1. Ingestion from Kafka ----------\r\n\r\nval kafkaBootstrap \u003d \"localhost:9092\"\r\nval topic \u003d \"electricity_topic\"\r\n\r\nval rawKafka \u003d spark.readStream\r\n  .format(\"kafka\")\r\n  .option(\"kafka.bootstrap.servers\", kafkaBootstrap)\r\n  .option(\"subscribe\", topic)\r\n  .option(\"startingOffsets\", \"latest\")\r\n  .load()\r\n\r\n// JSON schema from your message\r\nval jsonSchema \u003d StructType(Array(\r\n  StructField(\"State_Code\", StringType),\r\n  StructField(\"Timestamp_UTC\", StringType),\r\n  StructField(\"Gross_Load_MW\", StringType),\r\n  StructField(\"Hour_Of_Day\", StringType),\r\n  StructField(\"Day_Of_Week\", StringType),\r\n  StructField(\"Is_Weekend\", StringType),\r\n  StructField(\"Is_Holiday_State\", StringType),\r\n  StructField(\"Avg_Temp_C\", StringType),\r\n  StructField(\"Temp_Change_6H\", StringType),\r\n  StructField(\"Avg_Humidity_Pct\", StringType)\r\n))\r\n\r\n// Parse JSON payload\r\nval rawParsed \u003d rawKafka\r\n  .selectExpr(\"CAST(value AS STRING) AS json\")\r\n  .select(from_json(col(\"json\"), jsonSchema).as(\"data\"))\r\n  .select(\"data.*\")\r\n\r\n// Basic type casting\r\nval typed \u003d rawParsed\r\n  .withColumn(\"Gross_Load_MW\", col(\"Gross_Load_MW\").cast(DoubleType))\r\n  .withColumn(\"Hour_Of_Day\", col(\"Hour_Of_Day\").cast(IntegerType))\r\n  .withColumn(\"Day_Of_Week\", col(\"Day_Of_Week\").cast(IntegerType))\r\n  .withColumn(\"Is_Weekend\", col(\"Is_Weekend\").cast(IntegerType))\r\n  .withColumn(\"Is_Holiday_State\", col(\"Is_Holiday_State\").cast(IntegerType))\r\n  .withColumn(\"Avg_Temp_C\", col(\"Avg_Temp_C\").cast(DoubleType))\r\n  .withColumn(\"Temp_Change_6H\", col(\"Temp_Change_6H\").cast(DoubleType))\r\n  .withColumn(\"Avg_Humidity_Pct\", col(\"Avg_Humidity_Pct\").cast(DoubleType))\r\n\r\n// ---------- 1.5 PREPROCESSING / DATA QUALITY (ROW-LEVEL ONLY) ----------\r\n\r\n// Helper: define reasonable ranges (domain knowledge)\r\nval minLoad \u003d 0.0\r\nval maxLoad \u003d 1e6\r\nval minTemp \u003d -30.0\r\nval maxTemp \u003d 55.0\r\nval minHumidity \u003d 0.0\r\nval maxHumidity \u003d 100.0\r\n\r\n// Treat NaNs as nulls\r\nval cleanedNulls \u003d typed\r\n  .withColumn(\"Gross_Load_MW\",\r\n    when(col(\"Gross_Load_MW\").isNull || col(\"Gross_Load_MW\").isNaN, lit(null).cast(DoubleType))\r\n      .otherwise(col(\"Gross_Load_MW\"))\r\n  )\r\n  .withColumn(\"Avg_Temp_C\",\r\n    when(col(\"Avg_Temp_C\").isNull || col(\"Avg_Temp_C\").isNaN, lit(null).cast(DoubleType))\r\n      .otherwise(col(\"Avg_Temp_C\"))\r\n  )\r\n  .withColumn(\"Temp_Change_6H\",\r\n    when(col(\"Temp_Change_6H\").isNull || col(\"Temp_Change_6H\").isNaN, lit(null).cast(DoubleType))\r\n      .otherwise(col(\"Temp_Change_6H\"))\r\n  )\r\n  .withColumn(\"Avg_Humidity_Pct\",\r\n    when(col(\"Avg_Humidity_Pct\").isNull || col(\"Avg_Humidity_Pct\").isNaN, lit(null).cast(DoubleType))\r\n      .otherwise(col(\"Avg_Humidity_Pct\"))\r\n  )\r\n\r\n// Remove obviously invalid categorical / timestamp values\r\nval filteredInvalid \u003d cleanedNulls\r\n  .filter(col(\"State_Code\").isNotNull \u0026\u0026 col(\"Timestamp_UTC\").isNotNull)\r\n  .filter(col(\"Hour_Of_Day\").between(0, 23))\r\n  .filter(col(\"Day_Of_Week\").between(0, 6))\r\n\r\n// Handle negative or unrealistic loads by clipping and flagging\r\nval clippedLoad \u003d filteredInvalid\r\n  .withColumn(\r\n    \"Gross_Load_MW_clipped\",\r\n    when(col(\"Gross_Load_MW\").isNull, null)\r\n      .when(col(\"Gross_Load_MW\") \u003c minLoad, minLoad)\r\n      .when(col(\"Gross_Load_MW\") \u003e maxLoad, maxLoad)\r\n      .otherwise(col(\"Gross_Load_MW\"))\r\n  )\r\n  .withColumn(\r\n    \"Load_Anomaly_Flag\",\r\n    when(col(\"Gross_Load_MW\").lt(minLoad) || col(\"Gross_Load_MW\").gt(maxLoad), lit(1))\r\n      .otherwise(lit(0))\r\n  )\r\n\r\n// Clip temperature and humidity to realistic ranges, flag anomalies\r\nval clippedWeather \u003d clippedLoad\r\n  .withColumn(\r\n    \"Avg_Temp_C_clipped\",\r\n    when(col(\"Avg_Temp_C\").isNull, null)\r\n      .when(col(\"Avg_Temp_C\") \u003c minTemp, minTemp)\r\n      .when(col(\"Avg_Temp_C\") \u003e maxTemp, maxTemp)\r\n      .otherwise(col(\"Avg_Temp_C\"))\r\n  )\r\n  .withColumn(\r\n    \"Avg_Humidity_Pct_clipped\",\r\n    when(col(\"Avg_Humidity_Pct\").isNull, null)\r\n      .when(col(\"Avg_Humidity_Pct\") \u003c minHumidity, minHumidity)\r\n      .when(col(\"Avg_Humidity_Pct\") \u003e maxHumidity, maxHumidity)\r\n      .otherwise(col(\"Avg_Humidity_Pct\"))\r\n  )\r\n  .withColumn(\r\n    \"Weather_Anomaly_Flag\",\r\n    when(col(\"Avg_Temp_C\").lt(minTemp) || col(\"Avg_Temp_C\").gt(maxTemp) ||\r\n         col(\"Avg_Humidity_Pct\").lt(minHumidity) || col(\"Avg_Humidity_Pct\").gt(maxHumidity), lit(1))\r\n      .otherwise(lit(0))\r\n  )\r\n\r\n// Simple per-row \"imputed\" values (no aggregation inside stream)\r\nval cleanDf \u003d clippedWeather\r\n  .withColumn(\"Gross_Load_MW_imputed\", col(\"Gross_Load_MW_clipped\"))\r\n  .withColumn(\"Avg_Temp_C_imputed\", col(\"Avg_Temp_C_clipped\"))\r\n  .withColumn(\"Avg_Humidity_Pct_imputed\", col(\"Avg_Humidity_Pct_clipped\"))\r\n  .withColumn(\"event_time\", to_timestamp(col(\"Timestamp_UTC\"), \"yyyy-MM-dd HH:mm:ss\"))\r\n  .withColumn(\"date\", date_format(col(\"event_time\"), \"yyyy-MM-dd\"))\r\n  .withColumn(\"hour\", date_format(col(\"event_time\"), \"HH\"))\r\n\r\n// optional: drop intermediate clipped columns if you don\u0027t need them\r\n//  .drop(\"Gross_Load_MW_clipped\", \"Avg_Temp_C_clipped\", \"Avg_Humidity_Pct_clipped\")\r\n\r\ncleanDf.printSchema()\r\n\r\n// ---------- 2. Storage: write clean data to versioned Parquet (append) ----------\r\n\r\nval parquetSink \u003d cleanDf.writeStream\r\n  .format(\"parquet\")\r\n  .option(\"path\", outputPath)\r\n  .option(\"checkpointLocation\", ckPath)\r\n  .partitionBy(\"date\", \"hour\")\r\n  .outputMode(\"append\")\r\n  .start()\r\n\r\nparquetSink"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nimport sys\r\nimport os\r\nimport re\r\nimport pandas as pd\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.window import Window\r\nfrom pyspark.sql.functions import pandas_udf\r\nfrom pyspark.sql.types import DoubleType\r\n\r\n\r\n\r\nrun_id \u003d \"v2\"  # manual override, if needed\r\n\r\ninput_path \u003d f\"/home/devpandya/parquet_clean_{run_id}\"\r\noutput_path \u003d f\"/home/devpandya/parquet_with_predictions_{run_id}\"\r\n\r\nprint(\"Reading clean parquet from:\", input_path)\r\n\r\n# -------------------------------------------------------------------\r\n# 2. Import model helper and read data\r\n# -------------------------------------------------------------------\r\n\r\nsys.path.append(\"/home/devpandya/bigdata-stack\")\r\nfrom model_inference_helper import ElectricityLoadPredictor\r\n\r\ndf \u003d spark.read.parquet(input_path)\r\n\r\nprint(\"Rows in clean parquet:\", df.count())\r\ndf.show(5, truncate\u003dFalse)\r\n\r\nbase_col \u003d \"Gross_Load_MW_imputed\"\r\n\r\n# -------------------------------------------------------------------\r\n# 3. Create lag and rolling features\r\n# -------------------------------------------------------------------\r\n\r\nw \u003d Window.partitionBy(\"State_Code\").orderBy(\"event_time\")\r\n\r\ndf_feat \u003d (\r\n    df\r\n    .withColumn(\"Load_lag_1h\",   F.lag(F.col(base_col), 1).over(w))\r\n    .withColumn(\"Load_lag_2h\",   F.lag(F.col(base_col), 2).over(w))\r\n    .withColumn(\"Load_lag_3h\",   F.lag(F.col(base_col), 3).over(w))\r\n    .withColumn(\"Load_lag_24h\",  F.lag(F.col(base_col), 24).over(w))\r\n    .withColumn(\"Load_lag_168h\", F.lag(F.col(base_col), 168).over(w))\r\n    .withColumn(\r\n        \"Load_roll_mean_3h\",\r\n        F.avg(F.col(base_col)).over(w.rowsBetween(-3, -1))\r\n    )\r\n    .withColumn(\r\n        \"Load_roll_mean_24h\",\r\n        F.avg(F.col(base_col)).over(w.rowsBetween(-24, -1))\r\n    )\r\n    .withColumn(\r\n        \"Load_roll_mean_168h\",\r\n        F.avg(F.col(base_col)).over(w.rowsBetween(-168, -1))\r\n    )\r\n)\r\n\r\n# Require only the \"short\" lags/rolls to be present\r\nrequired_cols_strict \u003d [\r\n    \"Load_lag_1h\", \"Load_lag_2h\", \"Load_lag_3h\",\r\n    \"Load_lag_24h\",\r\n    \"Load_roll_mean_3h\", \"Load_roll_mean_24h\",\r\n]\r\n\r\ncond \u003d F.lit(True)\r\nfor c in required_cols_strict:\r\n    cond \u003d cond \u0026 F.col(c).isNotNull()\r\n\r\ndf_feat \u003d df_feat.filter(cond)\r\n\r\n# For 168h features, approximate missing values with 24h features\r\ndf_feat \u003d (\r\n    df_feat\r\n    .withColumn(\r\n        \"Load_lag_168h\",\r\n        F.when(F.col(\"Load_lag_168h\").isNull(), F.col(\"Load_lag_24h\"))\r\n         .otherwise(F.col(\"Load_lag_168h\"))\r\n    )\r\n    .withColumn(\r\n        \"Load_roll_mean_168h\",\r\n        F.when(F.col(\"Load_roll_mean_168h\").isNull(), F.col(\"Load_roll_mean_24h\"))\r\n         .otherwise(F.col(\"Load_roll_mean_168h\"))\r\n    )\r\n)\r\n\r\n\r\nprint(\"Rows after feature engineering:\", df_feat.count())\r\ndf_feat.select(\r\n    \"State_Code\", \"Timestamp_UTC\", base_col,\r\n    \"Load_lag_1h\", \"Load_lag_2h\", \"Load_lag_3h\",\r\n    \"Load_lag_24h\", \"Load_lag_168h\",\r\n    \"Load_roll_mean_3h\", \"Load_roll_mean_24h\", \"Load_roll_mean_168h\"\r\n).show(5, truncate\u003dFalse)\r\n\r\npredictor \u003d ElectricityLoadPredictor()\r\n\r\n# -------------------------------------------------------------------\r\n# 4. pandas UDF building feature matrix as in training\r\n# -------------------------------------------------------------------\r\n\r\n@pandas_udf(DoubleType())\r\ndef predict_udf(pdf: pd.DataFrame) -\u003e pd.Series:\r\n    X \u003d pd.DataFrame()\r\n    X[\"State_Code\"] \u003d pdf[\"State_Code\"]\r\n    X[\"Hour_Of_Day\"] \u003d pdf[\"Hour_Of_Day\"]\r\n    X[\"Day_Of_Week\"] \u003d pdf[\"Day_Of_Week\"]\r\n    X[\"Is_Weekend\"] \u003d pdf[\"Is_Weekend\"]\r\n    X[\"Is_Holiday_State\"] \u003d pdf[\"Is_Holiday_State\"]\r\n    X[\"Avg_Temp_C\"] \u003d pdf[\"Avg_Temp_C_imputed\"]\r\n    X[\"Temp_Change_6H\"] \u003d pdf[\"Temp_Change_6H\"]\r\n    X[\"Avg_Humidity_Pct\"] \u003d pdf[\"Avg_Humidity_Pct_imputed\"]\r\n\r\n    for col in [\r\n        \"Load_lag_1h\", \"Load_lag_2h\", \"Load_lag_3h\",\r\n        \"Load_lag_24h\", \"Load_lag_168h\",\r\n        \"Load_roll_mean_3h\", \"Load_roll_mean_24h\", \"Load_roll_mean_168h\",\r\n    ]:\r\n        X[col] \u003d pdf[col]\r\n\r\n    preds \u003d predictor.predict_batch(X.to_dict(orient\u003d\"records\"))\r\n    return pd.Series(preds)\r\n\r\ndf_pred \u003d df_feat.withColumn(\r\n    \"Predicted_Load_MW\",\r\n    predict_udf(F.struct([F.col(c) for c in df_feat.columns]))\r\n)\r\n\r\ndf_pred.show(10, truncate\u003dFalse)\r\n\r\nprint(\"Writing predictions to:\", output_path)\r\n(\r\n    df_pred\r\n    .write\r\n    .mode(\"overwrite\")\r\n    .partitionBy(\"date\", \"hour\")\r\n    .parquet(output_path)\r\n)\r\n\r\nprint(\"Done.\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval runId    \u003d \"v2\"  // keep in sync with what you used / auto-detected\r\nval predPath \u003d s\"/home/devpandya/parquet_with_predictions_$runId\"\r\n\r\nval predDf \u003d spark.read.parquet(predPath)\r\n\r\nprintln(\"Rows with predictions: \" + predDf.count())\r\npredDf.show(20, truncate\u003dfalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\nval tsAgg \u003d predDf\r\n  .groupBy(\"State_Code\", \"date\", \"hour\")\r\n  .agg(\r\n    avg(\"Gross_Load_MW\").alias(\"Avg_Actual_Load\"),\r\n    avg(\"Predicted_Load_MW\").alias(\"Avg_Predicted_Load\")\r\n  )\r\n  .orderBy(\"State_Code\", \"date\", \"hour\")\r\n\r\ntsAgg.show(100, truncate\u003dfalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nimport org.apache.spark.sql.functions._\r\n\r\nval withError \u003d predDf\r\n  .withColumn(\"abs_error\", abs(col(\"Gross_Load_MW\") - col(\"Predicted_Load_MW\")))\r\n  .withColumn(\r\n    \"ape\",\r\n    when(col(\"Gross_Load_MW\") \u003d!\u003d 0.0,\r\n         abs(col(\"Gross_Load_MW\") - col(\"Predicted_Load_MW\")) / abs(col(\"Gross_Load_MW\")))\r\n      .otherwise(lit(null))\r\n  )\r\n\r\nval errorByState \u003d withError\r\n  .groupBy(\"State_Code\")\r\n  .agg(\r\n    avg(\"abs_error\").alias(\"MAE\"),\r\n    avg(\"ape\").alias(\"MAPE\")\r\n  )\r\n  .orderBy(\"State_Code\")\r\n\r\nerrorByState.show(truncate\u003dfalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n\r\nval exportDf \u003d predDf.select(\r\n  \"State_Code\",\r\n  \"Timestamp_UTC\",\r\n  \"Gross_Load_MW\",\r\n  \"Predicted_Load_MW\",\r\n  \"Avg_Temp_C\",\r\n  \"Avg_Humidity_Pct\",\r\n  \"Is_Weekend\"\r\n)\r\n\r\nval exportPath \u003d \"/home/devpandya/powerbi_export_csv\"\r\n\r\nexportDf\r\n  .coalesce(1)\r\n  .write\r\n  .mode(\"overwrite\")\r\n  .option(\"header\", \"true\")\r\n  .csv(exportPath)\r\n\r\nprintln(s\"Exported for Power BI to: $exportPath\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval runId    \u003d \"v1\"  // or whatever version you are using\nval exportPath \u003d s\"/mnt/c/Users/DEV/Desktop/Minor Project/Project/powerbi_export_v${runId}\"\n\n// Select the columns you want for analysis / Power BI\nval exportDf \u003d predDf.select(\n  \"State_Code\",\n  \"Timestamp_UTC\",\n  \"Gross_Load_MW\",\n  \"Predicted_Load_MW\",\n  \"Avg_Temp_C\",\n  \"Avg_Humidity_Pct\",\n  \"Is_Weekend\"\n)\n\n// Write as a single CSV file (inside a folder) with header\nexportDf\n  .coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(exportPath)\n\nprintln(s\"Exported for Power BI to Windows path mounted at: $exportPath\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}