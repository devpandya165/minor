#######################################################################################
phase 1 {run kafka dummy msgs from producer to consumer}
#######################################################################################


## ğŸ“Œ What You Did Till Now

1. **Installed WSL2 + Ubuntu** â†’ gave you a Linux environment inside Windows.
2. **Installed Docker Desktop** with WSL2 integration.
3. **Installed Docker Compose** and ran `docker run hello-world` to verify.
4. **Created a project folder** (`~/bigdata-stack`) with `docker-compose.yml`.
5. **Started containers** (`docker-compose up -d`) â†’ launched:

   * Zookeeper
   * Kafka
   * MongoDB
   * InfluxDB
   * Grafana
6. **Verified containers** with `docker ps`.
7. **Tested Kafka**:

   * Created a topic.
   * Ran a producer (`kafka-console-producer`).
   * Ran a consumer (`kafka-console-consumer`).
   * Confirmed messages are flowing from producer â†’ consumer âœ….

---

## ğŸ”„ How to Restart Everything (without me)

Whenever you reboot or shut down, follow this exact sequence:

1. **Start Ubuntu (WSL2)**
   Open PowerShell or Windows Terminal â†’ run:

   ```powershell
   wsl -d Ubuntu
   ```

2. **Go to your project folder**

   ```bash
   cd ~/bigdata-stack
   ```

3. **Start all services**

   ```bash
   docker-compose up -d
   ```

4. **Check containers**

   ```bash
   docker ps
   ```

   Make sure you see **zookeeper, kafka, mongodb, influxdb, grafana** running.

---

## ğŸ“¨ Producer â†’ Consumer Flow

After services are up:

### 1. Create topic (only once per new setup):

```bash
docker exec -it kafka kafka-topics --create \
  --topic test-topic \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 1
```

### 2. Start producer (terminal 1):

```bash
docker exec -it kafka kafka-console-producer \
  --topic test-topic \
  --bootstrap-server localhost:9092
```

Now type messages and press **Enter**.

### 3. Start consumer (terminal 2 â†’ also run Ubuntu with `wsl -d Ubuntu`):

```bash
docker exec -it kafka kafka-console-consumer \
  --topic test-topic \
  --from-beginning \
  --bootstrap-server localhost:9092
```

Youâ€™ll see the producerâ€™s messages appear here âœ….

---

## ğŸ”Œ Stop Everything

When youâ€™re done:

```bash
cd ~/bigdata-stack
docker-compose down
```

This will cleanly stop all containers.

---

ğŸ‘‰ With this sequence, you can **bring up the full stack, send messages, and consume them anytime**.

Do you want me to now give you a **Python script** that does the producer/consumer part automatically (instead of typing messages manually)?




#######################################################################################
phase 2 kafka io to kafka io (producer consumer)
#######################################################################################

Absolutely âœ… â€” thatâ€™s *exactly* what you should do first.
Creating a **dummy producerâ€“consumer pipeline** is the best way to get comfortable with Kafka before connecting it to MongoDB.

Hereâ€™s the plan ğŸ‘‡

---

## ğŸ§© Dummy Data Streaming Workflow

**Goal:**
Send fake (dummy) energy data â†’ through Kafka producer â†’ read it with Kafka consumer â†’ print it live on terminal.

No databases involved yet â€” just Kafka I/O.

---

### **1ï¸âƒ£ Create a Kafka Topic**

Run this inside Ubuntu:

```bash
docker exec -it kafka kafka-topics --create \
  --topic dummy_energy \
  --bootstrap-server localhost:9092 \
  --partitions 1 --replication-factor 1
```

Youâ€™ll get a success message confirming topic creation.

---

### **2ï¸âƒ£ Create a Dummy Producer Script**

Create file `producer.py` in your Ubuntu home directory:

```bash
nano producer.py
```

Paste this code:

```python
from kafka import KafkaProducer
import json, time, random

# connect to Kafka
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

# list of states
states = ["Gujarat", "Maharashtra", "Tamil Nadu", "Rajasthan", "Karnataka"]

while True:
    data = {
        "state": random.choice(states),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "demand_mw": round(random.uniform(1000, 8000), 2),
        "renewable_share": round(random.uniform(10, 90), 2)
    }

    producer.send('dummy_energy', value=data)
    print(f"Sent: {data}")
    time.sleep(2)
```

Save with **Ctrl+O**, Enter, **Ctrl+X**.

Run it:

```bash
python3 producer.py
```

Youâ€™ll see lines like:

```
Sent: {'state': 'Gujarat', 'timestamp': '2025-10-08 18:05:03', 'demand_mw': 5278.5, 'renewable_share': 43.6}
```

---

### **3ï¸âƒ£ Create a Dummy Consumer Script**

In another terminal:

```bash
nano consumer.py
```

Paste:

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'dummy_energy',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='energy_group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

print("Listening for messages...\n")

for message in consumer:
    data = message.value
    print(f"Received: {data}")
```

Save and run:

```bash
python3 consumer.py
```

---

### **4ï¸âƒ£ Observe the Flow**

* The producer keeps sending dummy state data every 2 seconds.
* The consumer prints every message as soon as itâ€™s received.

âœ… **Congrats!** Youâ€™ve just built a real-time data stream using Kafka.

---

### **5ï¸âƒ£ When Done**

* Stop scripts using `Ctrl + C`
* You can stop all containers with:

  ```bash
  docker-compose down
  ```

---

Would you like me to extend this dummy setup next into a **MongoDB â†’ Kafka â†’ MongoDB** version once you confirm this part runs fine?


